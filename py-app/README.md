# Python Web Scraping + FastAPI App

This project is a Python web scraping and text processing service built with FastAPI.
It includes:
	â€¢	ğŸŒ Crawler â†’ Sitemap + spider crawling
	â€¢	ğŸ§¹ Extractor â†’ HTML parsing & text cleaning
	â€¢	ğŸ” Embeddings â†’ Generate embeddings for scraped text
	â€¢	ğŸ“¦ Storage â†’ Save embeddings into VectorDB/FAISS
	â€¢	âš¡ Tasks â†’ Async background workers for long-running jobs

## Project Structure
```
py-app/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI entry point
â”‚   â”œâ”€â”€ models.py        # Pydantic request/response models
â”‚   â”œâ”€â”€ crawler.py       # Sitemap & spider crawling logic
â”‚   â”œâ”€â”€ extractor.py     # HTML parsing & text cleaning
â”‚   â”œâ”€â”€ embeddings.py    # LLM embedding functions
â”‚   â”œâ”€â”€ storage.py       # Vector DB or FAISS storage
â”‚   â””â”€â”€ tasks.py         # Async background tasks
â”‚
â”‚â”€â”€ requirements.txt     # Python dependencies
â”‚â”€â”€ README.md            # Project documentation
```

## Getting Started

#### 1. Clone the Repo
```
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>/app/py-app
```

#### 2. Create Virtual Environment
```
python3 -m venv .venv
source .venv/bin/activate   # Mac/Linux
.venv\Scripts\activate      # Windows
```

#### 3. Install Dependencies
```
pip install -r requirements.txt
```

#### 4. Run the FastAPI Server
```
uvicorn app.main:app --reload
```

#### Server will start at:
ğŸ‘‰ http://127.0.0.1:8000

#### Interactive API docs:
ğŸ‘‰ http://127.0.0.1:8000/docs

## Example Endpoints
	â€¢	POST /crawl â†’ Start crawling a sitemap or domain
	â€¢	GET /extract/{url} â†’ Extract and clean HTML from a given URL
	â€¢	POST /embed â†’ Generate embeddings from text
	â€¢	GET /search?query=... â†’ Search stored embeddings in vector DB

## Dependencies
	â€¢	FastAPI â€“ web framework
	â€¢	Uvicorn â€“ ASGI server
	â€¢	BeautifulSoup4 / lxml â€“ HTML parsing
	â€¢	Requests / httpx â€“ HTTP requests
	â€¢	FAISS / Chroma / Pinecone â€“ Vector DB storage (choose one)
	â€¢	Pydantic â€“ request/response validation

## Development Notes
	â€¢	Keep all scraping logic inside crawler.py & extractor.py.
	â€¢	Embeddings and vector storage are modular â†’ swap FAISS with any other DB.
	â€¢	Async background tasks (tasks.py) can be run with Celery or RQ if scaling is needed.

## Future Improvements
	â€¢	Add Dockerfile for containerized deployment
	â€¢	Add scheduler for recurring scraping jobs
	â€¢	Add authentication for API endpoints
